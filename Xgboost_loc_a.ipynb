{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import TimeSeriesSplit \n",
    "import optuna\n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "#Load inn datasets\n",
    "X_test  = pd.read_parquet('data/prepared_datasets/avg/no_duplicates/X_test.parquet')\n",
    "X_train = pd.read_parquet('data/prepared_datasets/avg/no_duplicates/X_train.parquet')\n",
    "y_train = pd.read_parquet('data/prepared_datasets/avg/no_duplicates/Y_train.parquet')\n",
    "y_train_a = pd.read_parquet('data/prepared_datasets/avg/no_duplicates/Y_train_a.parquet')\n",
    "y_train_b = pd.read_parquet('data/prepared_datasets/avg/no_duplicates/Y_train_b.parquet')\n",
    "y_train_c = pd.read_parquet('data/prepared_datasets/avg/no_duplicates/Y_train_c.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(df):\n",
    "    dates_2 = (df.index >= '2023-04-01') & (df.index <= '2023-04-15')\n",
    "    dates_1 = (df.index >= '2021-05-01') & (df.index <= '2021-08-01')\n",
    "\n",
    "    test_set = df[dates_1 | dates_2]\n",
    "\n",
    "    training_set = df[~(dates_1 | dates_2)]\n",
    "\n",
    "    X_train = training_set.drop(\"pv_measurement\", axis=1)\n",
    "    y_train = training_set['pv_measurement']\n",
    "\n",
    "    X_test = test_set.drop(\"pv_measurement\", axis=1)\n",
    "    y_test = test_set['pv_measurement'] \n",
    "\n",
    "    \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train_new_a, X_test_new_a, y_train_new_a, y_test_a = test_train_split(pd.concat([X_train[X_train[\"location\"] == \"A\"].drop(\"location\", axis=1), y_train_a], axis=1))\n",
    "X_train_new_b, X_test_new_b, y_train_new_b, y_test_b = test_train_split(pd.concat([X_train[X_train[\"location\"] == \"B\"].drop(\"location\", axis=1), y_train_b], axis=1))\n",
    "X_train_new_c, X_test_new_c, y_train_new_c, y_test_c = test_train_split(pd.concat([X_train[X_train[\"location\"] == \"C\"].drop(\"location\", axis=1), y_train_c], axis=1))\n",
    "\n",
    "X_train_loc_a, X_test_loc_a, y_train_loc_a, y_test_a = test_train_split(pd.concat([X_train[X_train[\"location\"] == \"A\"], y_train_a], axis=1))\n",
    "X_train_loc_b, X_test_loc_b, y_train_loc_b, y_test_b = test_train_split(pd.concat([X_train[X_train[\"location\"] == \"B\"], y_train_b], axis=1))\n",
    "X_train_loc_c, X_test_loc_c, y_train_loc_c, y_test_c = test_train_split(pd.concat([X_train[X_train[\"location\"] == \"C\"], y_train_c], axis=1))\n",
    "\n",
    "\n",
    "X_train_new = pd.concat([X_train_loc_a, X_train_loc_b, X_train_loc_c])\n",
    "X_test_new = pd.concat([X_test_loc_a, X_test_loc_b, X_test_loc_c])\n",
    "y_train_new = pd.concat([y_train_loc_a, y_train_loc_b, y_train_loc_c])\n",
    "y_test = pd.concat([y_test_a, y_test_b, y_test_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-11 18:04:59,537] A new study created in memory with name: no-name-04a57b40-6451-438c-8e24-c4a42713941b\n",
      "[I 2023-11-11 18:05:11,117] Trial 0 finished with value: 330.4766048363645 and parameters: {'n_estimators': 1043, 'learning_rate': 0.024890512306631928, 'max_depth': 10, 'subsample': 0.807006709281653, 'colsample_bytree': 0.405480948435344, 'min_child_weight': 19}. Best is trial 0 with value: 330.4766048363645.\n",
      "[I 2023-11-11 18:05:15,024] Trial 1 finished with value: 471.4937668960224 and parameters: {'n_estimators': 1410, 'learning_rate': 0.0014457415839290405, 'max_depth': 2, 'subsample': 0.25334115313753564, 'colsample_bytree': 0.9076842908234225, 'min_child_weight': 4}. Best is trial 0 with value: 330.4766048363645.\n",
      "[I 2023-11-11 18:05:39,458] Trial 2 finished with value: 337.3198771475631 and parameters: {'n_estimators': 2020, 'learning_rate': 0.019615279921596136, 'max_depth': 9, 'subsample': 0.5729242078418975, 'colsample_bytree': 0.7807147757058316, 'min_child_weight': 9}. Best is trial 0 with value: 330.4766048363645.\n",
      "[I 2023-11-11 18:05:50,286] Trial 3 finished with value: 340.8288746747969 and parameters: {'n_estimators': 2398, 'learning_rate': 0.0025435091687053533, 'max_depth': 5, 'subsample': 0.4503555555179417, 'colsample_bytree': 0.20064966514354293, 'min_child_weight': 20}. Best is trial 0 with value: 330.4766048363645.\n",
      "[I 2023-11-11 18:05:54,944] Trial 4 finished with value: 355.11582347818774 and parameters: {'n_estimators': 1448, 'learning_rate': 0.09976952200486158, 'max_depth': 3, 'subsample': 0.5613677164459341, 'colsample_bytree': 0.40371988539859355, 'min_child_weight': 17}. Best is trial 0 with value: 330.4766048363645.\n",
      "[I 2023-11-11 18:06:04,029] Trial 5 finished with value: 331.63431977798507 and parameters: {'n_estimators': 1025, 'learning_rate': 0.008047988091693851, 'max_depth': 9, 'subsample': 0.2630922198654261, 'colsample_bytree': 0.26296037509423964, 'min_child_weight': 15}. Best is trial 0 with value: 330.4766048363645.\n",
      "[I 2023-11-11 18:06:09,080] Trial 6 finished with value: 351.33913902318335 and parameters: {'n_estimators': 1336, 'learning_rate': 0.06159672578480972, 'max_depth': 4, 'subsample': 0.4393783759598355, 'colsample_bytree': 0.7703098356564914, 'min_child_weight': 16}. Best is trial 0 with value: 330.4766048363645.\n",
      "[I 2023-11-11 18:06:16,488] Trial 7 finished with value: 420.0026085294273 and parameters: {'n_estimators': 1836, 'learning_rate': 0.07317479668131374, 'max_depth': 5, 'subsample': 0.09843476100836722, 'colsample_bytree': 0.23916422668671922, 'min_child_weight': 2}. Best is trial 0 with value: 330.4766048363645.\n",
      "[I 2023-11-11 18:06:24,176] Trial 8 finished with value: 412.2044246098832 and parameters: {'n_estimators': 2855, 'learning_rate': 0.0010899553812063324, 'max_depth': 2, 'subsample': 0.21962190624107863, 'colsample_bytree': 0.7380891573605615, 'min_child_weight': 15}. Best is trial 0 with value: 330.4766048363645.\n",
      "[I 2023-11-11 18:06:30,165] Trial 9 finished with value: 359.41226517849935 and parameters: {'n_estimators': 2281, 'learning_rate': 0.012540079491760654, 'max_depth': 2, 'subsample': 0.3449780063627398, 'colsample_bytree': 0.8789902799321758, 'min_child_weight': 8}. Best is trial 0 with value: 330.4766048363645.\n",
      "[I 2023-11-11 18:06:36,029] Trial 10 finished with value: 401.6122017673359 and parameters: {'n_estimators': 1058, 'learning_rate': 0.02636570017522644, 'max_depth': 7, 'subsample': 0.85010608709432, 'colsample_bytree': 0.05167507523995041, 'min_child_weight': 20}. Best is trial 0 with value: 330.4766048363645.\n",
      "[I 2023-11-11 18:06:52,073] Trial 11 finished with value: 326.651819573843 and parameters: {'n_estimators': 1040, 'learning_rate': 0.006004368057548323, 'max_depth': 10, 'subsample': 0.9924312878199646, 'colsample_bytree': 0.509751031413391, 'min_child_weight': 13}. Best is trial 11 with value: 326.651819573843.\n",
      "[I 2023-11-11 18:07:14,406] Trial 12 finished with value: 329.57402419747245 and parameters: {'n_estimators': 1612, 'learning_rate': 0.006626239926790066, 'max_depth': 10, 'subsample': 0.9618783088549442, 'colsample_bytree': 0.5163714091325077, 'min_child_weight': 12}. Best is trial 11 with value: 326.651819573843.\n",
      "[I 2023-11-11 18:07:28,155] Trial 13 finished with value: 328.8528308363954 and parameters: {'n_estimators': 1725, 'learning_rate': 0.005860847641993863, 'max_depth': 7, 'subsample': 0.9424859710887711, 'colsample_bytree': 0.5570127511745402, 'min_child_weight': 12}. Best is trial 11 with value: 326.651819573843.\n",
      "[I 2023-11-11 18:07:50,771] Trial 14 finished with value: 328.47593538135635 and parameters: {'n_estimators': 2968, 'learning_rate': 0.0035820431445604288, 'max_depth': 7, 'subsample': 0.9546780606994623, 'colsample_bytree': 0.6422737106006067, 'min_child_weight': 12}. Best is trial 11 with value: 326.651819573843.\n",
      "[I 2023-11-11 18:08:14,670] Trial 15 finished with value: 327.53863811522785 and parameters: {'n_estimators': 2849, 'learning_rate': 0.0029642843600737596, 'max_depth': 7, 'subsample': 0.7558362161605175, 'colsample_bytree': 0.6022776440154576, 'min_child_weight': 6}. Best is trial 11 with value: 326.651819573843.\n",
      "[I 2023-11-11 18:09:50,240] Trial 16 finished with value: 326.73121344876205 and parameters: {'n_estimators': 2621, 'learning_rate': 0.0029904816911802716, 'max_depth': 8, 'subsample': 0.7184444049473866, 'colsample_bytree': 0.6112519572222339, 'min_child_weight': 7}. Best is trial 11 with value: 326.651819573843.\n",
      "[I 2023-11-11 18:10:06,281] Trial 17 finished with value: 326.9281799713699 and parameters: {'n_estimators': 2612, 'learning_rate': 0.004292487495567724, 'max_depth': 8, 'subsample': 0.6911490168870532, 'colsample_bytree': 0.46356168338199366, 'min_child_weight': 6}. Best is trial 11 with value: 326.651819573843.\n",
      "[I 2023-11-11 18:10:29,166] Trial 18 finished with value: 333.17586174176046 and parameters: {'n_estimators': 2123, 'learning_rate': 0.0019657406020490266, 'max_depth': 9, 'subsample': 0.671052257739033, 'colsample_bytree': 0.7081924921216032, 'min_child_weight': 10}. Best is trial 11 with value: 326.651819573843.\n",
      "[W 2023-11-11 18:10:36,393] Trial 19 failed with parameters: {'n_estimators': 2540, 'learning_rate': 0.004400707818871094, 'max_depth': 8, 'subsample': 0.9996674499012124, 'colsample_bytree': 0.986942941726568, 'min_child_weight': 1} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Shahl\\AppData\\Local\\Temp\\ipykernel_4132\\161067834.py\", line 24, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, X_train_new_a, y_train_new_a), n_trials=100)\n",
      "  File \"C:\\Users\\Shahl\\AppData\\Local\\Temp\\ipykernel_4132\\161067834.py\", line 18, in objective\n",
      "    model.fit(X_train_new_a, y_train_new_a, verbose=300)\n",
      "  File \"c:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py\", line 1086, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"c:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py\", line 2050, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2023-11-11 18:10:36,401] Trial 19 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m MAE\n\u001b[0;32m     23\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m study\u001b[39m.\u001b[39;49moptimize(\u001b[39mlambda\u001b[39;49;00m trial: objective(trial, X_train_new_a, y_train_new_a), n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[4], line 24\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m MAE\n\u001b[0;32m     23\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m study\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: objective(trial, X_train_new_a, y_train_new_a), n_trials\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, X_train_new, y_train_new)\u001b[0m\n\u001b[0;32m      6\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m\"\u001b[39m: trial\u001b[39m.\u001b[39msuggest_int(\u001b[39m\"\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1000\u001b[39m, \u001b[39m3000\u001b[39m),\n\u001b[0;32m      8\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mverbosity\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[39m'\u001b[39m\u001b[39meval_metric\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     15\u001b[0m }\n\u001b[0;32m     17\u001b[0m model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m---> 18\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_new_a, y_train_new_a, verbose\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m)\n\u001b[0;32m     19\u001b[0m pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test_new_a)\n\u001b[0;32m     20\u001b[0m MAE \u001b[39m=\u001b[39m mean_absolute_error(y_test_a, pred)\n",
      "File \u001b[1;32mc:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:1086\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1075\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m (\n\u001b[0;32m   1078\u001b[0m     model,\n\u001b[0;32m   1079\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1084\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1085\u001b[0m )\n\u001b[1;32m-> 1086\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1087\u001b[0m     params,\n\u001b[0;32m   1088\u001b[0m     train_dmatrix,\n\u001b[0;32m   1089\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1090\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1091\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1092\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1093\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1094\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1095\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1096\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1097\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1098\u001b[0m )\n\u001b[0;32m   1100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Shahl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:2050\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2048\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2049\u001b[0m     _check_call(\n\u001b[1;32m-> 2050\u001b[0m         _LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\n\u001b[0;32m   2051\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, ctypes\u001b[39m.\u001b[39;49mc_int(iteration), dtrain\u001b[39m.\u001b[39;49mhandle\n\u001b[0;32m   2052\u001b[0m         )\n\u001b[0;32m   2053\u001b[0m     )\n\u001b[0;32m   2054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2055\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "\n",
    "def objective(trial, X_train_new, y_train_new):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 3000),\n",
    "        \"verbosity\": 0,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "        'eval_metric': 'mae'\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train_new_a, y_train_new_a, verbose=300)\n",
    "    pred = model.predict(X_test_new_a)\n",
    "    MAE = mean_absolute_error(y_test_a, pred)\n",
    "    return MAE\n",
    "    \n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, X_train_new_a, y_train_new_a), n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
