{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The short notebook for submission 1\n",
    "- Team Name: Random Forest Rangers\n",
    "- Member 1: Andr√©s Zambrano / 122532\n",
    "- Member 2: Mikael Aleksander Jansen Shahly / 544897"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Notebook will contain loading inn the data, all changes to the data etc to make the necessary training sets. \n",
    "It will then cover making the models and making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#Setting max display options to avoid local crashes\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_a = pd.read_parquet('../data/A/train_targets.parquet')\n",
    "Y_train_b = pd.read_parquet('../data/B/train_targets.parquet')\n",
    "Y_train_c = pd.read_parquet('../data/C/train_targets.parquet')\n",
    "X_train_estimated_a = pd.read_parquet('../data/A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('../data/B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('../data/C/X_train_estimated.parquet')\n",
    "X_train_observed_a = pd.read_parquet('../data/A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('../data/B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('../data/C/X_train_observed.parquet')\n",
    "X_test_estimated_a = pd.read_parquet('../data/A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('../data/B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('../data/C/X_test_estimated.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making dataset for location A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the X dataset for location A\n",
    "X_train_estimated_a[\"estimated\"] = 1.0\n",
    "X_test_estimated_a[\"estimated\"] = 1.0\n",
    "X_train_observed_a[\"estimated\"] = 0.0\n",
    "X_train_observed_a[\"date_calc\"] = X_train_observed_a[\"date_forecast\"].copy() #date calc column added to observed data so that observed and estimated has equal nr cols\n",
    "\n",
    "X_train_estimated_a[\"train/test\"] = \"train\"\n",
    "X_train_observed_a[\"train/test\"] = \"train\"\n",
    "X_test_estimated_a[\"train/test\"] = \"test\"\n",
    "\n",
    "X_all_a= pd.concat([X_train_observed_a, X_train_estimated_a, X_test_estimated_a], ignore_index=True)\n",
    "X_all_a.set_index('date_forecast', inplace=True)\n",
    "\n",
    "#Create the complete train dataset for location A\n",
    "train_all_a = pd.concat([X_train_observed_a.drop(\"train/test\", axis=1), X_train_estimated_a.drop(\"train/test\", axis=1)], ignore_index=True)\n",
    "train_all_a.set_index('date_forecast', inplace=True)\n",
    "\n",
    "#remove all non hourly values\n",
    "train_all_a = train_all_a[train_all_a.index.minute == 0]\n",
    "\n",
    "#Set the index as the date forecast for y aswell\n",
    "Y_train_a.set_index('time', inplace=True)\n",
    "\n",
    "#Concatenate with y data\n",
    "train_all_a = pd.concat([train_all_a, Y_train_a], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making dataset for location B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all data from B\n",
    "X_train_estimated_b[\"estimated\"] = 1.0\n",
    "X_test_estimated_b[\"estimated\"] = 1.0\n",
    "X_train_observed_b[\"estimated\"] = 0.0\n",
    "X_train_observed_b[\"date_calc\"] = X_train_observed_b[\"date_forecast\"].copy()\n",
    "\n",
    "X_train_estimated_b[\"train/test\"] = \"train\"\n",
    "X_train_observed_b[\"train/test\"] = \"train\"\n",
    "X_test_estimated_b[\"train/test\"] = \"test\"\n",
    "\n",
    "X_all_b= pd.concat([X_train_observed_b, X_train_estimated_b, X_test_estimated_b], ignore_index=True)\n",
    "X_all_b.set_index('date_forecast', inplace=True)\n",
    "\n",
    "#Create the complete train dataset for location B\n",
    "train_all_b = pd.concat([X_train_observed_b.drop(\"train/test\", axis=1), X_train_estimated_b.drop(\"train/test\", axis=1)], ignore_index=True)\n",
    "train_all_b.set_index('date_forecast', inplace=True)\n",
    "#remove all non hourly values\n",
    "train_all_b = train_all_b[train_all_b.index.minute == 0]\n",
    "\n",
    "#Set the index as the date forecast for y aswell\n",
    "Y_train_b.set_index('time', inplace=True)\n",
    "\n",
    "#Concatenate with y data\n",
    "train_all_b = pd.concat([train_all_b, Y_train_b], axis=1)\n",
    "#Drop the first row, as it is not in the X data\n",
    "train_all_b.drop(\"2018-12-31 23:00:00\", inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete C dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all data from C\n",
    "X_train_estimated_c[\"estimated\"] = 1.0\n",
    "X_test_estimated_c[\"estimated\"] = 1.0\n",
    "X_train_observed_c[\"estimated\"] = 0.0\n",
    "X_train_observed_c[\"date_calc\"] = X_train_observed_c[\"date_forecast\"].copy()\n",
    "X_train_estimated_c[\"train/test\"] = \"train\"\n",
    "X_train_observed_c[\"train/test\"] = \"train\"\n",
    "X_test_estimated_c[\"train/test\"] = \"test\"\n",
    "\n",
    "X_all_c= pd.concat([X_train_observed_c, X_train_estimated_c, X_test_estimated_c], ignore_index=True)\n",
    "X_all_c.set_index('date_forecast', inplace=True)\n",
    "\n",
    "#Create the complete train dataset for location A\n",
    "train_all_c = pd.concat([X_train_observed_c.drop(\"train/test\", axis=1), X_train_estimated_c.drop(\"train/test\", axis=1)], ignore_index=True)\n",
    "train_all_c.set_index('date_forecast', inplace=True)\n",
    "#remove all non hourly values\n",
    "train_all_c = train_all_c[train_all_c.index.minute == 0]\n",
    "\n",
    "#Set the index as the date forecast for y aswell\n",
    "Y_train_c.set_index('time', inplace=True)\n",
    "\n",
    "#Concatenate with y data\n",
    "train_all_c = pd.concat([train_all_c, Y_train_c], axis=1)\n",
    "\n",
    "train_all_c.drop(\"2018-12-31 23:00:00\", inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make combined datasets for all locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_a = X_all_a.copy()\n",
    "copy_b = X_all_b.copy()\n",
    "copy_c = X_all_c.copy()\n",
    "\n",
    "copy_a[\"location\"] = \"A\"\n",
    "copy_b[\"location\"] = \"B\"\n",
    "copy_c[\"location\"] = \"C\"\n",
    "X_all = pd.concat([copy_a, copy_b, copy_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_a_train = train_all_a.copy()\n",
    "copy_b_train = train_all_b.copy()\n",
    "copy_c_train = train_all_c.copy()\n",
    "\n",
    "copy_a_train[\"location\"] = \"A\"\n",
    "copy_b_train[\"location\"] = \"B\"\n",
    "copy_c_train[\"location\"] = \"C\"\n",
    "\n",
    "train_all = pd.concat([copy_a_train, copy_b_train, copy_c_train])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a test-set for all locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_all[X_all[\"train/test\"] == \"test\"]\n",
    "X_test = X_test.drop(\"train/test\", axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function for resampling to average X-values from the 15 minute intervals - called train_all_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_df_no_categorical(df, hourly_df):\n",
    "    indices_to_keep = hourly_df.index\n",
    "    resampled_df = df.resample('1H').mean()\n",
    "    resampled_df = resampled_df[resampled_df.index.isin(indices_to_keep)]\n",
    "    return resampled_df\n",
    "\n",
    "#use to create averages\n",
    "train_all_avg_a = resample_df_no_categorical(X_all_a.drop([\"train/test\"], axis = 1), train_all_a)\n",
    "train_all_avg_b = resample_df_no_categorical(X_all_b.drop([\"train/test\"], axis = 1), train_all_b)\n",
    "train_all_avg_c = resample_df_no_categorical(X_all_c.drop([\"train/test\"], axis = 1), train_all_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add back locations\n",
    "train_all_avg_a[\"location\"] = \"A\"\n",
    "train_all_avg_b[\"location\"] = \"B\"\n",
    "train_all_avg_c[\"location\"] = \"C\"\n",
    "\n",
    "#add in pv measurements\n",
    "train_all_avg_c = pd.concat([train_all_avg_c, Y_train_c], axis=1)\n",
    "train_all_avg_b = pd.concat([train_all_avg_b, Y_train_b], axis=1)\n",
    "train_all_avg_a = pd.concat([train_all_avg_a, Y_train_a], axis=1)\n",
    "\n",
    "train_all_avg = pd.concat([train_all_avg_a, train_all_avg_b,train_all_avg_c])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function to remove the duplicates in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_over_threshold(df, threshold, col):\n",
    "    threshold # Set your threshold here\n",
    "\n",
    "    # Create a boolean mask to identify consecutive duplicates (non-zero values)\n",
    "    mask = df[col].ne(df[col].shift()).cumsum()\n",
    "\n",
    "    # Create a mask to identify rows that need to be kept (consecutive zeros, first in a group, or the first row of a group)\n",
    "    rank_within_group = df.groupby(mask)[col].rank(method='first')\n",
    "    keep_mask = (df[col] == 0) | (rank_within_group <= threshold - 1) | (mask != mask.shift())\n",
    "\n",
    "    # Filter the DataFrame based on the keep_mask\n",
    "    df_filtered = df[keep_mask]\n",
    "    return df_filtered\n",
    "\n",
    "train_all_avg = remove_duplicates_over_threshold(train_all_avg, 10, \"pv_measurement\").copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make another function to remove all values under a certain thershold that is repeated (typically when sensor is covered in snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_over_threshold_under_val(df, col, threshold, val):\n",
    "    # Define the threshold\n",
    "    threshold\n",
    "\n",
    "    # Create a boolean mask for values less than 1\n",
    "    mask = df[col] == 0\n",
    "\n",
    "    # Use cumulative sum to identify consecutive groups of values less than 1\n",
    "    groups = (mask != mask.shift()).cumsum()\n",
    "\n",
    "    # Filter out groups that don't meet the threshold\n",
    "    valid_groups = groups[mask].value_counts() >= threshold\n",
    "    valid_mask = groups.map(valid_groups.get).fillna(False)\n",
    "\n",
    "    # Select rows that meet the criteria\n",
    "    filtered_df = df[~valid_mask]\n",
    "    return filtered_df\n",
    "\n",
    "train_all_avg_no_lows = remove_duplicates_over_threshold_under_val(train_all_avg, \"pv_measurement\", 40, 1).copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping rows containing only Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all.drop(train_all[train_all[\"absolute_humidity_2m:gm3\"].isnull()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_avg.drop(train_all_avg[train_all_avg[\"absolute_humidity_2m:gm3\"].isnull()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_avg_no_lows.drop(train_all_avg_no_lows[train_all_avg_no_lows[\"absolute_humidity_2m:gm3\"].isnull()].index, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop snow denisity col as it contains almost only Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = train_all.drop([\"snow_density:kgm3\"], axis=1)\n",
    "train_all_avg = train_all_avg.drop([\"snow_density:kgm3\"], axis=1)\n",
    "train_all_avg_no_lows = train_all_avg_no_lows.drop([\"snow_density:kgm3\"], axis=1)\n",
    "\n",
    "\n",
    "X_test= X_test.drop([\"snow_density:kgm3\"], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all rows with Nans in target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = train_all.dropna(axis=0, subset=[\"pv_measurement\"])\n",
    "train_all_avg = train_all_avg.dropna(axis=0, subset=[\"pv_measurement\"])\n",
    "train_all_avg_no_lows = train_all_avg_no_lows.dropna(axis=0, subset=[\"pv_measurement\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the fully to be used in the ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_non_avg = train_all.drop([\"pv_measurement\", \"date_calc\"], axis=1)\n",
    "y_train_non_avg= train_all[['pv_measurement']]\n",
    "\n",
    "X_train_avg = train_all_avg.drop([\"pv_measurement\", \"date_calc\"], axis=1)\n",
    "y_train_avg= train_all_avg[['pv_measurement']]\n",
    "\n",
    "X_train_avg_no_lows = train_all_avg_no_lows.drop([\"pv_measurement\", \"date_calc\"], axis=1)\n",
    "y_train_avg_no_lows = train_all_avg_no_lows[['pv_measurement']]\n",
    "\n",
    "X_test = X_test.drop(\"date_calc\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting out y-train values per location\n",
    "y_train_a_avg = train_all_avg[train_all_avg[\"location\"] == \"A\"][[\"pv_measurement\"]]\n",
    "y_train_b_avg = train_all_avg[train_all_avg[\"location\"] == \"B\"][[\"pv_measurement\"]]\n",
    "y_train_c_avg = train_all_avg[train_all_avg[\"location\"] == \"C\"][[\"pv_measurement\"]]\n",
    "\n",
    "y_train_a_non_avg = train_all[train_all[\"location\"] == \"A\"][[\"pv_measurement\"]]\n",
    "y_train_b_non_avg = train_all[train_all[\"location\"] == \"B\"][[\"pv_measurement\"]]\n",
    "y_train_c_non_avg = train_all[train_all[\"location\"] == \"C\"][[\"pv_measurement\"]]\n",
    "\n",
    "y_train_a_avg_no_lows = train_all_avg_no_lows[train_all_avg_no_lows[\"location\"] == \"A\"][[\"pv_measurement\"]]\n",
    "y_train_b_avg_no_lows = train_all_avg_no_lows[train_all_avg_no_lows[\"location\"] == \"B\"][[\"pv_measurement\"]]\n",
    "y_train_c_avg_no_lows = train_all_avg_no_lows[train_all_avg_no_lows[\"location\"] == \"C\"][[\"pv_measurement\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now create the models and stack them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first models used in our stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 288.4237594\ttotal: 357ms\tremaining: 5m 56s\n",
      "100:\tlearn: 95.0993860\ttotal: 31.5s\tremaining: 4m 40s\n",
      "200:\tlearn: 85.7607295\ttotal: 1m 4s\tremaining: 4m 15s\n",
      "300:\tlearn: 83.1566183\ttotal: 1m 36s\tremaining: 3m 44s\n",
      "400:\tlearn: 81.0675620\ttotal: 2m 10s\tremaining: 3m 14s\n",
      "500:\tlearn: 79.1934025\ttotal: 2m 43s\tremaining: 2m 42s\n",
      "600:\tlearn: 77.3369244\ttotal: 3m 17s\tremaining: 2m 11s\n",
      "700:\tlearn: 75.6892747\ttotal: 3m 52s\tremaining: 1m 39s\n",
      "800:\tlearn: 73.9179017\ttotal: 4m 27s\tremaining: 1m 6s\n",
      "900:\tlearn: 72.2539595\ttotal: 5m 2s\tremaining: 33.3s\n",
      "999:\tlearn: 70.8529308\ttotal: 5m 37s\tremaining: 0us\n",
      "0:\tlearn: 280.0059853\ttotal: 271ms\tremaining: 4m 30s\n",
      "100:\tlearn: 99.0865828\ttotal: 31.2s\tremaining: 4m 37s\n",
      "200:\tlearn: 90.2153171\ttotal: 1m 3s\tremaining: 4m 14s\n",
      "300:\tlearn: 87.5726426\ttotal: 1m 37s\tremaining: 3m 45s\n",
      "400:\tlearn: 85.6303550\ttotal: 2m 10s\tremaining: 3m 14s\n",
      "500:\tlearn: 83.7731682\ttotal: 2m 44s\tremaining: 2m 44s\n",
      "600:\tlearn: 82.4051164\ttotal: 3m 19s\tremaining: 2m 12s\n",
      "700:\tlearn: 81.1314369\ttotal: 3m 56s\tremaining: 1m 40s\n",
      "800:\tlearn: 79.6476444\ttotal: 4m 31s\tremaining: 1m 7s\n",
      "900:\tlearn: 78.3427872\ttotal: 5m 8s\tremaining: 33.9s\n",
      "999:\tlearn: 77.2839505\ttotal: 5m 44s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1ee034e20b0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting up avg / non-avg whole dataset\n",
    "\n",
    "#Create a pool of data\n",
    "train_pool_avg = Pool(X_train_avg, y_train_avg, cat_features=[\"location\"])\n",
    "test_pool_avg = Pool(X_test, cat_features=[\"location\"]) \n",
    "\n",
    "train_pool_non_avg = Pool(X_train_non_avg, y_train_non_avg, cat_features=[\"location\"])\n",
    "test_pool_non_avg = Pool(X_test, cat_features=[\"location\"]) \n",
    "\n",
    "#init models and fit them\n",
    "catboost_model_avg = CatBoostRegressor(iterations=1000, depth=9, loss_function=\"LogCosh\", verbose=100, random_seed=0)\n",
    "catboost_model_avg.fit(train_pool_avg)\n",
    "\n",
    "catboost_model_non_avg = CatBoostRegressor(iterations=1000, depth=9, loss_function=\"LogCosh\", verbose=100, random_seed=0)\n",
    "catboost_model_non_avg.fit(train_pool_non_avg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second models used in our stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 318.9293862\ttotal: 299ms\tremaining: 4m 58s\n",
      "100:\tlearn: 103.0052888\ttotal: 29.2s\tremaining: 4m 20s\n",
      "200:\tlearn: 92.9501070\ttotal: 59.5s\tremaining: 3m 56s\n",
      "300:\tlearn: 89.7083996\ttotal: 1m 30s\tremaining: 3m 29s\n",
      "400:\tlearn: 87.6226020\ttotal: 2m 1s\tremaining: 3m 1s\n",
      "500:\tlearn: 85.6117466\ttotal: 2m 34s\tremaining: 2m 34s\n",
      "600:\tlearn: 83.8118371\ttotal: 3m 8s\tremaining: 2m 4s\n",
      "700:\tlearn: 81.7809387\ttotal: 3m 41s\tremaining: 1m 34s\n",
      "800:\tlearn: 80.0318201\ttotal: 4m 14s\tremaining: 1m 3s\n",
      "900:\tlearn: 78.5463802\ttotal: 4m 45s\tremaining: 31.4s\n",
      "999:\tlearn: 77.1956032\ttotal: 5m 17s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1ee26c23af0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add in dataset with lows removed\n",
    "train_pool_avg_no_lows = Pool(X_train_avg_no_lows, y_train_avg_no_lows, cat_features=[\"location\"])\n",
    "test_pool = Pool(X_test, cat_features=[\"location\"]) \n",
    "\n",
    "catboost_model_avg_no_lows = CatBoostRegressor(iterations=1000, depth=9, loss_function=\"LogCosh\", verbose=100, random_seed=0)\n",
    "catboost_model_avg_no_lows.fit(train_pool_avg_no_lows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third models used in our stack\n",
    "- Note this uses previoudsly found hyperparamaters with optuna. The code for finding these hyperparams is given at the end of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 624.9320516\ttotal: 80.4ms\tremaining: 3m 21s\n",
      "400:\tlearn: 183.8413421\ttotal: 34.2s\tremaining: 3m\n",
      "800:\tlearn: 169.7375507\ttotal: 1m 8s\tremaining: 2m 26s\n",
      "1200:\tlearn: 155.6490973\ttotal: 1m 43s\tremaining: 1m 52s\n",
      "1600:\tlearn: 146.4125240\ttotal: 2m 18s\tremaining: 1m 18s\n",
      "2000:\tlearn: 139.1288148\ttotal: 2m 48s\tremaining: 42.9s\n",
      "2400:\tlearn: 130.4735681\ttotal: 3m 9s\tremaining: 8.66s\n",
      "2510:\tlearn: 129.1970689\ttotal: 3m 15s\tremaining: 0us\n",
      "0:\tlearn: 96.2305278\ttotal: 12.3ms\tremaining: 20.3s\n",
      "400:\tlearn: 45.4338503\ttotal: 5.4s\tremaining: 16.8s\n",
      "800:\tlearn: 42.8962877\ttotal: 11s\tremaining: 11.6s\n",
      "1200:\tlearn: 41.9358083\ttotal: 16.8s\tremaining: 6.22s\n",
      "1600:\tlearn: 41.2727568\ttotal: 22.7s\tremaining: 651ms\n",
      "1646:\tlearn: 41.2349579\ttotal: 23.4s\tremaining: 0us\n",
      "0:\tlearn: 75.7193866\ttotal: 50.2ms\tremaining: 1m 51s\n",
      "400:\tlearn: 15.2136583\ttotal: 17.9s\tremaining: 1m 21s\n",
      "800:\tlearn: 11.5094546\ttotal: 33.9s\tremaining: 59.8s\n",
      "1200:\tlearn: 8.6439264\ttotal: 48.6s\tremaining: 41.1s\n",
      "1600:\tlearn: 7.0792931\ttotal: 1m 3s\tremaining: 24.4s\n",
      "2000:\tlearn: 5.9914399\ttotal: 1m 17s\tremaining: 8.35s\n",
      "2215:\tlearn: 5.4836251\ttotal: 1m 25s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1ee26c237f0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create training location per location with prev found hyperparamaters focusing on 2020 summer (the old old ones)\n",
    "params_a = {'iterations': 2511, 'learning_rate': 0.013387708538234228, 'depth': 9, 'min_data_in_leaf': 37, 'l2_leaf_reg': 9, 'bagging_temperature': 0.6037942407543951, 'random_strength': 0.7203046909719719, 'border_count': 792, 'rsm': 0.5142593972884377, \"loss_function\": \"LogCosh\"}\n",
    "params_b = {'iterations': 1647, 'learning_rate': 0.008285667949530987, 'depth': 2, 'min_data_in_leaf': 7, 'l2_leaf_reg': 6, 'bagging_temperature': 0.638344903820208, 'random_strength': 0.9388444992830671, 'border_count': 434, 'rsm': 0.8347735297026142, \"loss_function\": \"LogCosh\"}\n",
    "params_c = {'iterations': 2216, 'learning_rate': 0.02932152249119453, 'depth': 10, 'min_data_in_leaf': 21, 'l2_leaf_reg': 2, 'bagging_temperature': 0.8784102456931138, 'random_strength': 0.6901129919784297, 'border_count': 119, 'rsm': 0.8571604856673344, \"loss_function\": \"LogCosh\"}\n",
    "\n",
    "X_train_a = X_train_non_avg[X_train_non_avg[\"location\"] == \"A\"].drop(\"location\", axis=1)\n",
    "X_train_b = X_train_non_avg[X_train_non_avg[\"location\"] == \"B\"].drop(\"location\", axis=1)\n",
    "X_train_c = X_train_non_avg[X_train_non_avg[\"location\"] == \"C\"].drop(\"location\", axis=1)\n",
    "\n",
    "X_test_a = X_test[X_test[\"location\"] == \"A\"].drop(\"location\", axis=1)\n",
    "X_test_b = X_test[X_test[\"location\"] == \"B\"].drop(\"location\", axis=1)\n",
    "X_test_c = X_test[X_test[\"location\"] == \"C\"].drop(\"location\", axis=1)\n",
    "          \n",
    "train_pool_a = Pool(X_train_a, y_train_a_non_avg)\n",
    "train_pool_b = Pool(X_train_b, y_train_b_non_avg)\n",
    "train_pool_c = Pool(X_train_c, y_train_c_non_avg)\n",
    "\n",
    "\n",
    "test_pool_a = Pool(X_test_a) \n",
    "test_pool_b = Pool(X_test_b) \n",
    "test_pool_c = Pool(X_test_c) \n",
    "\n",
    "catboost_model_a = CatBoostRegressor(**params_a, verbose = 400, random_seed=0)\n",
    "catboost_model_b = CatBoostRegressor(**params_b, verbose = 400, random_seed=0)\n",
    "catboost_model_c = CatBoostRegressor(**params_c, verbose = 400, random_seed=0)\n",
    "\n",
    "catboost_model_a.fit(train_pool_a)\n",
    "catboost_model_b.fit(train_pool_b)\n",
    "catboost_model_c.fit(train_pool_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 626.4702501\ttotal: 29.8ms\tremaining: 1m 8s\n",
      "400:\tlearn: 194.8882584\ttotal: 14.9s\tremaining: 1m 10s\n",
      "800:\tlearn: 182.6842082\ttotal: 30.4s\tremaining: 56.7s\n",
      "1200:\tlearn: 175.2218005\ttotal: 45.9s\tremaining: 41.7s\n",
      "1600:\tlearn: 169.9364448\ttotal: 1m 1s\tremaining: 26.6s\n",
      "2000:\tlearn: 164.9970920\ttotal: 1m 17s\tremaining: 11.3s\n",
      "2292:\tlearn: 161.3938650\ttotal: 1m 29s\tremaining: 0us\n",
      "0:\tlearn: 95.3837887\ttotal: 12.6ms\tremaining: 30.2s\n",
      "400:\tlearn: 37.0045121\ttotal: 5.7s\tremaining: 28.3s\n",
      "800:\tlearn: 35.2342283\ttotal: 11.7s\tremaining: 23.3s\n",
      "1200:\tlearn: 34.0703961\ttotal: 17.8s\tremaining: 17.6s\n",
      "1600:\tlearn: 33.2272447\ttotal: 24s\tremaining: 11.8s\n",
      "2000:\tlearn: 32.4425948\ttotal: 30.2s\tremaining: 5.91s\n",
      "2391:\tlearn: 31.8306956\ttotal: 36.5s\tremaining: 0us\n",
      "0:\tlearn: 77.3164904\ttotal: 53.1ms\tremaining: 2m 40s\n",
      "400:\tlearn: 46.2400201\ttotal: 23.9s\tremaining: 2m 36s\n",
      "800:\tlearn: 30.8360516\ttotal: 48.8s\tremaining: 2m 15s\n",
      "1200:\tlearn: 24.0644687\ttotal: 1m 13s\tremaining: 1m 51s\n",
      "1600:\tlearn: 20.9227284\ttotal: 1m 38s\tremaining: 1m 27s\n",
      "2000:\tlearn: 19.2023358\ttotal: 2m 2s\tremaining: 1m 2s\n",
      "2400:\tlearn: 18.1989792\ttotal: 2m 27s\tremaining: 38.3s\n",
      "2800:\tlearn: 17.4839876\ttotal: 2m 52s\tremaining: 13.8s\n",
      "3024:\tlearn: 17.1389162\ttotal: 3m 6s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1ee1e5d4d60>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best params for 2021 avg loc by loc\n",
    "params_avg_a = {'iterations': 2293, 'learning_rate': 0.008620188420007512, 'depth': 8, 'min_data_in_leaf': 18, 'l2_leaf_reg': 3, 'bagging_temperature': 0.4326298721660991, 'random_strength': 0.4658204470151926, 'border_count': 837, 'rsm': 0.7221138820493201, \"loss_function\": \"LogCosh\"}\n",
    "params_avg_b = {'iterations': 2392, 'learning_rate': 0.02247702224978235, 'depth': 5, 'min_data_in_leaf': 94, 'l2_leaf_reg': 4, 'bagging_temperature': 0.7611940774372695, 'random_strength': 0.44589753796694487, 'border_count': 151, 'rsm': 0.783116168473357, \"loss_function\": \"LogCosh\"}\n",
    "params_avg_c = {'iterations': 3025, 'learning_rate': 0.0017608361144341644, 'depth': 11, 'min_data_in_leaf': 18, 'l2_leaf_reg': 10, 'bagging_temperature': 0.7399543079455375, 'random_strength': 0.39726371700016055, 'border_count': 179, 'rsm': 0.6742668098873739, \"loss_function\": \"LogCosh\"}\n",
    "\n",
    "X_train_avg_a = X_train_avg[X_train_avg[\"location\"] == \"A\"].drop(\"location\", axis=1)\n",
    "X_train_avg_b = X_train_avg[X_train_avg[\"location\"] == \"B\"].drop(\"location\", axis=1)\n",
    "X_train_avg_c = X_train_avg[X_train_avg[\"location\"] == \"C\"].drop(\"location\", axis=1)\n",
    "\n",
    "X_test_a = X_test[X_test[\"location\"] == \"A\"].drop(\"location\", axis=1)\n",
    "X_test_b = X_test[X_test[\"location\"] == \"B\"].drop(\"location\", axis=1)\n",
    "X_test_c = X_test[X_test[\"location\"] == \"C\"].drop(\"location\", axis=1)\n",
    "          \n",
    "train_pool_avg_a = Pool(X_train_avg_a, y_train_a_avg)\n",
    "train_pool_avg_b = Pool(X_train_avg_b, y_train_b_avg)\n",
    "train_pool_avg_c = Pool(X_train_avg_c, y_train_c_avg)\n",
    "\n",
    "\n",
    "test_pool_a = Pool(X_test_a) \n",
    "test_pool_b = Pool(X_test_b) \n",
    "test_pool_c = Pool(X_test_c) \n",
    "\n",
    "catboost_model_avg_a_best_hyper = CatBoostRegressor(**params_avg_a, verbose=400, random_seed=0)\n",
    "catboost_model_avg_b_best_hyper = CatBoostRegressor(**params_avg_b, verbose=400, random_seed=0)\n",
    "catboost_model_avg_c_best_hyper = CatBoostRegressor(**params_avg_c, verbose=400, random_seed=0)\n",
    "\n",
    "catboost_model_avg_a_best_hyper.fit(train_pool_a)\n",
    "catboost_model_avg_b_best_hyper.fit(train_pool_b)\n",
    "catboost_model_avg_c_best_hyper.fit(train_pool_c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the predictions from our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_a = catboost_model_a.predict(test_pool_a)\n",
    "pred_b = catboost_model_b.predict(test_pool_b)\n",
    "pred_c = catboost_model_c.predict(test_pool_c)\n",
    "pred_non_avg_per_location_best_hyper = np.concatenate((pred_a, pred_b, pred_c))\n",
    "\n",
    "pred_non_avg_per_location_best_hyper[pred_non_avg_per_location_best_hyper < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_avg_a = catboost_model_avg_a_best_hyper.predict(test_pool_a)\n",
    "pred_avg_b = catboost_model_avg_b_best_hyper.predict(test_pool_b)\n",
    "pred_avg_c = catboost_model_avg_c_best_hyper.predict(test_pool_c)\n",
    "pred_avg_per_location_best_hyper = np.concatenate((pred_avg_a, pred_avg_b, pred_avg_c))\n",
    "\n",
    "pred_avg_per_location_best_hyper[pred_avg_per_location_best_hyper < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions for non-summer\n",
    "pred_avg = catboost_model_avg.predict(test_pool_avg)\n",
    "pred_non_avg = catboost_model_non_avg.predict(test_pool_non_avg)\n",
    "pred_avg[pred_avg < 0] = 0\n",
    "pred_non_avg[pred_non_avg < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions for non-summer avg with lows removed\n",
    "pred_avg_no_lows = catboost_model_avg.predict(test_pool)\n",
    "pred_avg_no_lows[pred_avg_no_lows < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tot_stacked = pd.DataFrame((pred_avg + pred_non_avg + pred_non_avg_per_location_best_hyper + pred_avg_per_location_best_hyper + pred_avg_no_lows)/5)\n",
    "\n",
    "predictions = pred_tot_stacked.clip(lower=0)\n",
    "\n",
    "def replace_under_0_2(x):\n",
    "    if x < 0.2:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "predictions = pred_tot_stacked.applymap(replace_under_0_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If wanted you can create a submission (as required for submitting to kaggle) below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 8640 entries, 2023-05-01 00:00:00 to 2023-07-03 23:45:00\n",
      "Data columns (total 46 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   absolute_humidity_2m:gm3        8640 non-null   float32\n",
      " 1   air_density_2m:kgm3             8640 non-null   float32\n",
      " 2   ceiling_height_agl:m            6243 non-null   float32\n",
      " 3   clear_sky_energy_1h:J           8640 non-null   float32\n",
      " 4   clear_sky_rad:W                 8640 non-null   float32\n",
      " 5   cloud_base_agl:m                7690 non-null   float32\n",
      " 6   dew_or_rime:idx                 8640 non-null   float32\n",
      " 7   dew_point_2m:K                  8640 non-null   float32\n",
      " 8   diffuse_rad:W                   8640 non-null   float32\n",
      " 9   diffuse_rad_1h:J                8640 non-null   float32\n",
      " 10  direct_rad:W                    8640 non-null   float32\n",
      " 11  direct_rad_1h:J                 8640 non-null   float32\n",
      " 12  effective_cloud_cover:p         8640 non-null   float32\n",
      " 13  elevation:m                     8640 non-null   float32\n",
      " 14  fresh_snow_12h:cm               8640 non-null   float32\n",
      " 15  fresh_snow_1h:cm                8640 non-null   float32\n",
      " 16  fresh_snow_24h:cm               8640 non-null   float32\n",
      " 17  fresh_snow_3h:cm                8640 non-null   float32\n",
      " 18  fresh_snow_6h:cm                8640 non-null   float32\n",
      " 19  is_day:idx                      8640 non-null   float32\n",
      " 20  is_in_shadow:idx                8640 non-null   float32\n",
      " 21  msl_pressure:hPa                8640 non-null   float32\n",
      " 22  precip_5min:mm                  8640 non-null   float32\n",
      " 23  precip_type_5min:idx            8640 non-null   float32\n",
      " 24  pressure_100m:hPa               8640 non-null   float32\n",
      " 25  pressure_50m:hPa                8640 non-null   float32\n",
      " 26  prob_rime:p                     8640 non-null   float32\n",
      " 27  rain_water:kgm2                 8640 non-null   float32\n",
      " 28  relative_humidity_1000hPa:p     8640 non-null   float32\n",
      " 29  sfc_pressure:hPa                8640 non-null   float32\n",
      " 30  snow_depth:cm                   8640 non-null   float32\n",
      " 31  snow_drift:idx                  8640 non-null   float32\n",
      " 32  snow_melt_10min:mm              8640 non-null   float32\n",
      " 33  snow_water:kgm2                 8640 non-null   float32\n",
      " 34  sun_azimuth:d                   8640 non-null   float32\n",
      " 35  sun_elevation:d                 8640 non-null   float32\n",
      " 36  super_cooled_liquid_water:kgm2  8640 non-null   float32\n",
      " 37  t_1000hPa:K                     8640 non-null   float32\n",
      " 38  total_cloud_cover:p             8640 non-null   float32\n",
      " 39  visibility:m                    8640 non-null   float32\n",
      " 40  wind_speed_10m:ms               8640 non-null   float32\n",
      " 41  wind_speed_u_10m:ms             8640 non-null   float32\n",
      " 42  wind_speed_v_10m:ms             8640 non-null   float32\n",
      " 43  wind_speed_w_1000hPa:ms         8640 non-null   float32\n",
      " 44  estimated                       8640 non-null   float64\n",
      " 45  location                        8640 non-null   object \n",
      "dtypes: float32(44), float64(1), object(1)\n",
      "memory usage: 1.6+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8640 entries, 0 to 8639\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   time        8640 non-null   datetime64[us]\n",
      " 1   prediction  8640 non-null   float64       \n",
      "dtypes: datetime64[us](1), float64(1)\n",
      "memory usage: 135.1 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def prepare_submission(predictions, X_test):\n",
    "    index_df = X_test.index.to_frame()\n",
    "    out_pd = pd.concat([index_df.reset_index(drop=True), predictions.reset_index(drop=True)], axis=1)\n",
    "    out_pd = out_pd.rename(columns={0: 'prediction', 'date_forecast': 'time'})\n",
    "    print(X_test.info())\n",
    "    print(out_pd.info())\n",
    "    out_pd['location'] = X_test['location'].reset_index(drop=True)\n",
    "    out_pd.set_index('time', inplace=True)\n",
    "    return out_pd\n",
    "\n",
    "def merge_with_sample(out_pd):\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    test.time = pd.to_datetime(test.time)\n",
    "    sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "    test.set_index('time', inplace=True)\n",
    "    \n",
    "    merged_df = test.reset_index().merge(out_pd.reset_index(), on=['time', 'location'], how='left', suffixes=('_original', '_new'))\n",
    "    merged_df['prediction_new'] = merged_df['prediction_new'].combine_first(merged_df['prediction_original'])\n",
    "    merged_df.drop('prediction_original', axis=1, inplace=True)\n",
    "    merged_df.rename(columns={'prediction_new': 'prediction'}, inplace=True)\n",
    "    return sample_submission[['id']].merge(merged_df[['id', 'prediction']], on='id', how='left')\n",
    "\n",
    "\n",
    "out_pd = prepare_submission(predictions, X_test)\n",
    "sample_submission = merge_with_sample(out_pd)\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code used to find hyper paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply change the dataframes in test_train_split function in order to get validation dataset for the avg values - used fof finding optimal hyperparams for avg values per location\n",
    "\n",
    "def test_train_split(df):\n",
    "    dates_2 = (df.index >= '2023-04-01') & (df.index <= '2023-04-15')\n",
    "    dates_1 = (df.index >= '2021-05-01') & (df.index <= '2021-08-01')\n",
    "\n",
    "    test_set = df[dates_1 | dates_2]\n",
    "\n",
    "    training_set = df[~(dates_1 | dates_2)]\n",
    "\n",
    "    X_train = training_set.drop(\"pv_measurement\", axis=1)\n",
    "    y_train = training_set['pv_measurement']\n",
    "\n",
    "    X_test = test_set.drop(\"pv_measurement\", axis=1)\n",
    "    y_test = test_set['pv_measurement'] \n",
    "\n",
    "    \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train_new_a, X_test_new_a, y_train_new_a, y_test_a = test_train_split(pd.concat([X_train_non_avg[X_train_non_avg[\"location\"] == \"A\"].drop(\"location\", axis=1), y_train_a_non_avg], axis=1))\n",
    "X_train_new_b, X_test_new_b, y_train_new_b, y_test_b = test_train_split(pd.concat([X_train_non_avg[X_train_non_avg[\"location\"] == \"B\"].drop(\"location\", axis=1), y_train_b_non_avg], axis=1))\n",
    "X_train_new_c, X_test_new_c, y_train_new_c, y_test_c = test_train_split(pd.concat([X_train_non_avg[X_train_non_avg[\"location\"] == \"C\"].drop(\"location\", axis=1), y_train_c_non_avg], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pool of data\n",
    "train_pool_a = Pool(X_train_new_a, y_train_new_a)\n",
    "train_pool_b = Pool(X_train_new_b, y_train_new_b)\n",
    "train_pool_c = Pool(X_train_new_c, y_train_new_c)\n",
    "\n",
    "\n",
    "test_pool_a = Pool(X_test_new_a) \n",
    "test_pool_b = Pool(X_test_new_b) \n",
    "test_pool_c = Pool(X_test_new_c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For location A\n",
    "def objective(trial, X_train, y_train):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 300, 3500),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 13),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 2, 10),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.3, 1.0),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.3, 1.0),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 1, 1000),\n",
    "        \"rsm\": trial.suggest_float(\"rsm\", 0.05, 1),\n",
    "        \"loss_function\" :\"LogCosh\"\n",
    "    }\n",
    "\n",
    "    catboost_model_a = CatBoostRegressor(**params, verbose=300)\n",
    "    catboost_model_a.fit(train_pool_a)\n",
    "    pred_a = pd.DataFrame(catboost_model_a.predict(test_pool_a))\n",
    "    MAE_a = mean_absolute_error(y_test_a, pred_a)\n",
    "    return MAE_a\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=80)\n",
    "\n",
    "\n",
    "#For location B\n",
    "def objective(trial, X_train, y_train):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 300, 3500),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 13),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 2, 10),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.3, 1.0),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.3, 1.0),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 1, 1000),\n",
    "        \"rsm\": trial.suggest_float(\"rsm\", 0.05, 1),\n",
    "        \"loss_function\" :\"LogCosh\"\n",
    "    }\n",
    "\n",
    "    catboost_model_c = CatBoostRegressor(**params, verbose=400)\n",
    "    catboost_model_c.fit(train_pool_c)\n",
    "    pred_c = pd.DataFrame(catboost_model_c.predict(test_pool_c))\n",
    "    MAE_c = mean_absolute_error(y_test_c, pred_c)\n",
    "    return MAE_c\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=80)\n",
    "\n",
    "\n",
    "#For location C\n",
    "def objective(trial, X_train, y_train):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 300, 3500),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 13),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 2, 10),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.3, 1.0),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.3, 1.0),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 1, 1000),\n",
    "        \"rsm\": trial.suggest_float(\"rsm\", 0.05, 1),\n",
    "        \"loss_function\" :\"LogCosh\"\n",
    "    }\n",
    "\n",
    "    catboost_model_b = CatBoostRegressor(**params, verbose=300)\n",
    "    catboost_model_b.fit(train_pool_b)\n",
    "    pred_b = pd.DataFrame(catboost_model_b.predict(test_pool_b))\n",
    "    MAE_b = mean_absolute_error(y_test_b, pred_b)\n",
    "    return MAE_b\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
