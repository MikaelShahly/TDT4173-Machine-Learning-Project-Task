{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import TimeSeriesSplit \n",
    "import optuna \n",
    "import json\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load inn datasets\n",
    "X_test  = pd.read_parquet('data/prepared_datasets/only_y_cleaned/X_test.parquet')\n",
    "X_train = pd.read_parquet('data/prepared_datasets/only_y_cleaned/X_train.parquet')\n",
    "y_train = pd.read_parquet('data/prepared_datasets/only_y_cleaned/Y_train.parquet')\n",
    "y_train_a = pd.read_parquet('data/prepared_datasets/only_y_cleaned/Y_train_a.parquet')\n",
    "y_train_b = pd.read_parquet('data/prepared_datasets/only_y_cleaned/Y_train_b.parquet')\n",
    "y_train_c = pd.read_parquet('data/prepared_datasets/only_y_cleaned/Y_train_c.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_def(df):\n",
    "    date_range_1 = (df.index >= '2020-05-01') & (df.index <= '2020-06-25')\n",
    "    date_range_2 = (df.index >= '2023-05-01') & (df.index <= '2023-06-15')\n",
    "\n",
    "    # Combine the date ranges to create the test set\n",
    "    test_set = df[date_range_1 | date_range_2]\n",
    "\n",
    "    # The rest of the data will be your training set\n",
    "    training_set = df[~(date_range_1 | date_range_2)]\n",
    "    \n",
    "    # Splitting the test_set into X_test and y_test\n",
    "    X_test = test_set.drop(\"pv_measurement\", axis=1)\n",
    "    y_test = test_set['pv_measurement']  # Assuming 'pv_measurement' is your target variable\n",
    "\n",
    "    # Splitting the training_set into X_train and y_train\n",
    "    X_train = training_set.drop(\"pv_measurement\", axis=1)\n",
    "    y_train = training_set['pv_measurement']\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train_new_a, X_test_new_a, y_train_new_a, y_test_a = splitting_def(pd.concat([X_train[X_train[\"location\"] == \"A\"].drop(\"location\", axis=1), y_train_a], axis=1))\n",
    "X_train_new_b, X_test_new_b, y_train_new_b, y_test_b = splitting_def(pd.concat([X_train[X_train[\"location\"] == \"B\"].drop(\"location\", axis=1), y_train_b], axis=1))\n",
    "X_train_new_c, X_test_new_c, y_train_new_c, y_test_c = splitting_def(pd.concat([X_train[X_train[\"location\"] == \"C\"].drop(\"location\", axis=1), y_train_c], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pool of data\n",
    "train_pool_a = Pool(X_train_new_a, y_train_new_a)\n",
    "train_pool_b = Pool(X_train_new_b, y_train_new_b)\n",
    "train_pool_c = Pool(X_train_new_c, y_train_new_c)\n",
    "\n",
    "\n",
    "test_pool_a = Pool(X_test_new_a) \n",
    "test_pool_b = Pool(X_test_new_b) \n",
    "test_pool_c = Pool(X_test_new_c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For location A\n",
    "def objective(trial, X_train, y_train):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 300, 3000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 13),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.05, 1.0),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 2, 10),\n",
    "        \"has-time\": trial.suggest_categorical('has-time', [True, False]),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.3, 1.0),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.3, 1.0),\n",
    "        \"random_state\": 2,\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 1, 500)\n",
    "    }\n",
    "\n",
    "    catboost_model_a = CatBoostRegressor(verbose=100)\n",
    "    catboost_model_a.fit(train_pool_a)\n",
    "    pred_a = pd.DataFrame(catboost_model_a.predict(test_pool_a))\n",
    "    MAE_a = mean_absolute_error(y_test_a, pred_a)\n",
    "    return MAE_a\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to output the best paramaters\n",
    "print(study.best_params)\n",
    "\n",
    "#to output the best score returned from the trials\n",
    "print(study.best_value)\n",
    "\n",
    "with open(\"optuna-best-parameters_a.txt\", \"w\") as file:\n",
    "    file.write(\"Best paramaters: \\n\")\n",
    "    file.write(json.dumps(study.best_params))  # Write the first string followed by a newline character\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"best score MAE: \\n\")\n",
    "    file.write(json.dumps(study.best_value))  # Write the second string followed by a newline character"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
